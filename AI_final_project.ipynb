{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIYGAqUHzBVFURsXogATQK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KalErgetu/Hybrid-AI-recommendation-system/blob/main/AI_final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q lightfm"
      ],
      "metadata": {
        "id": "dH4-GyXm1K9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTIRx78GtwK2",
        "outputId": "b1c73a97-999e-4dd6-a9fa-3efcb51c643a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "from lightfm import LightFM\n",
        "from lightfm.data import Dataset\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from lightfm.cross_validation import random_train_test_split\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "metadata": {
        "id": "Qd6rEy8_1rEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "\n",
        "path = '/content/drive/My Drive/AI/'\n",
        "\n",
        "Users = pd.read_csv(path + 'Users.csv')\n",
        "Vendors = pd.read_csv(path + 'Vendors.csv')\n",
        "Payments = pd.read_csv(path + 'Payments.csv')\n",
        "Bookings = pd.read_csv(path + 'Bookings.csv')\n",
        "\n",
        "# --- Data Cleaning ---\n",
        "def clean_data(Users, Vendors, Payments, Bookings):\n",
        "    \"\"\"Clean all input datasets\"\"\"\n",
        "    # [Insert the full cleaning code from previous message]\n",
        "    return Users, Vendors, Payments, Bookings\n",
        "\n",
        "Users, Vendors, Payments, Bookings = clean_data(Users, Vendors, Payments, Bookings)\n",
        "\n",
        "# --- Feature Enhancement ---\n",
        "def enhance_features(Users, Vendors):\n",
        "    \"\"\"Create additional useful features\"\"\"\n",
        "    # [Insert the full feature enhancement code from previous message]\n",
        "    return Users, Vendors\n",
        "\n",
        "Users, Vendors = enhance_features(Users, Vendors)\n",
        "\n",
        "print(\"Data loaded and cleaned successfully:\")\n",
        "print(\"Users:\")\n",
        "print(Users.head())\n",
        "print(\"Vendors:\")\n",
        "print(Vendors.head())\n",
        "print(\"Payments:\")\n",
        "print(Payments.head())\n",
        "print(\"Booking:\")\n",
        "print(Bookings.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gojAx082uPLe",
        "outputId": "0e8e5bf9-9751-4345-864f-30a7b0e2ff0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded and cleaned successfully:\n",
            "Users:\n",
            "   User_ID              Name                        Email  \\\n",
            "0        1    George English      clinenicholas@gmail.com   \n",
            "1        2        Amy Rhodes  nicholas65@barajas-rich.biz   \n",
            "2        3     Sandra Rivers              xray@morgan.net   \n",
            "3        4  Stephanie Wilson          christy02@gmail.com   \n",
            "4        5      John Mendoza            klevine@gmail.com   \n",
            "\n",
            "                   Phone     Location Event_Preference  \n",
            "0           065.175.4462  Addis Ababa     Chair Rental  \n",
            "1  +1-259-302-6313x06471  Addis Ababa  Audio Equipment  \n",
            "2      684.798.9144x4062  Addis Ababa     Wedding Hall  \n",
            "3           895-640-0921  Addis Ababa  Audio Equipment  \n",
            "4    +1-210-316-0175x471  Addis Ababa         Lighting  \n",
            "Vendors:\n",
            "   Vendor_ID              Business_Name      Category  Annual_Revenue_ETB  \\\n",
            "0          1   Ellis, Lopez and Johnson      Catering           120159.36   \n",
            "1          2  Park, Christian and Scott      Lighting            85833.21   \n",
            "2          3                   Soto Ltd      Catering           308985.67   \n",
            "3          4                  White LLC  Chair Rental            66853.44   \n",
            "4          5      Rush, Carney and Kidd      Lighting            72864.64   \n",
            "\n",
            "   Rating  \n",
            "0     3.2  \n",
            "1     4.2  \n",
            "2     3.9  \n",
            "3     4.6  \n",
            "4     3.4  \n",
            "Payments:\n",
            "   Payment_ID  User_ID  Vendor_ID Payment_Method  Transaction_Amount_ETB\n",
            "0           1    64078       2542    Credit Card                32154.78\n",
            "1           2    67619       1715    Credit Card                28638.04\n",
            "2           3    76812        253          Chapa                20587.46\n",
            "3           4    74231        177       Telebirr                 9477.45\n",
            "4           5    44354       1525       Telebirr                19083.02\n",
            "Booking:\n",
            "   Booking_ID  User_ID  Vendor_ID Booking_Date  Amount_Spent_ETB\n",
            "0           1    31064        977    3/16/2023          21092.21\n",
            "1           2    65275       3516    3/21/2021          48882.87\n",
            "2           3    82737       1494    4/25/2020          17285.83\n",
            "3           4     2855       1627   10/18/2021           9730.23\n",
            "4           5    22172       3233    9/13/2021           5362.53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced feature engineering for vendors\n",
        "def enhance_vendor_features(Vendors):\n",
        "    \"\"\"Create additional useful features for vendors\"\"\"\n",
        "    # Price range categorization\n",
        "    Vendors['Price_Category'] = pd.qcut(Vendors['Annual_Revenue_ETB'],\n",
        "                                      q=5,\n",
        "                                      labels=['Budget', 'Economy', 'Mid-Range', 'Premium', 'Luxury'])\n",
        "\n",
        "    # Availability score (based on booking frequency)\n",
        "    booking_frequency = Bookings.groupby('Vendor_ID').size()\n",
        "    Vendors['Availability_Score'] = 1 / (1 + booking_frequency.reindex(Vendors.Vendor_ID).fillna(0))\n",
        "\n",
        "    # Calculate vendor popularity score\n",
        "    Vendors['Popularity_Score'] = (\n",
        "        0.4 * Vendors['Rating'] +\n",
        "        0.3 * (Vendors['Annual_Revenue_ETB'] / Vendors['Annual_Revenue_ETB'].max()) +\n",
        "        0.3 * Vendors['Availability_Score']\n",
        "    )\n",
        "\n",
        "    return Vendors\n",
        "\n",
        "# Enhanced feature engineering for users\n",
        "def enhance_user_features(Users, Payments, Bookings):\n",
        "    \"\"\"Create additional useful features for users\"\"\"\n",
        "    # Combine user interactions\n",
        "    user_interactions = pd.concat([\n",
        "        Payments[['User_ID', 'Transaction_Amount_ETB']],\n",
        "        Bookings[['User_ID', 'Amount_Spent_ETB']].rename(columns={'Amount_Spent_ETB': 'Transaction_Amount_ETB'})\n",
        "    ])\n",
        "\n",
        "    # Calculate user spending patterns\n",
        "    spending_patterns = user_interactions.groupby('User_ID').agg({\n",
        "        'Transaction_Amount_ETB': ['mean', 'sum', 'count']\n",
        "    }).reset_index()\n",
        "    spending_patterns.columns = ['User_ID', 'Avg_Spending', 'Total_Spending', 'Transaction_Count']\n",
        "\n",
        "    # Merge with Users\n",
        "    Users = Users.merge(spending_patterns, on='User_ID', how='left')\n",
        "\n",
        "    # Create user segments based on spending\n",
        "    Users['User_Segment'] = pd.qcut(Users['Total_Spending'].fillna(0),\n",
        "                                  q=4,\n",
        "                                  labels=['Low', 'Medium', 'High', 'Premium'])\n",
        "\n",
        "    return Users"
      ],
      "metadata": {
        "id": "m2-lJS7o7BET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine interactions from payments and bookings\n",
        "interactions = pd.concat([\n",
        "    Payments[['User_ID', 'Vendor_ID', 'Transaction_Amount_ETB']].rename(columns={'Transaction_Amount_ETB': 'Interaction_Value'}),\n",
        "    Bookings[['User_ID', 'Vendor_ID', 'Amount_Spent_ETB']].rename(columns={'Amount_Spent_ETB': 'Interaction_Value'})\n",
        "])\n",
        "# After merging payments and bookings\n",
        "print(\"\\n=== Combined Interactions ===\")\n",
        "print(\"Merged Payment + Booking Sample:\")\n",
        "print(interactions.head())\n",
        "print(\"Shape of interactions_df:\", interactions.shape)\n",
        "\n",
        "# Sum interaction values (can be frequency, avg, or sum depending on use case)\n",
        "interaction_grouped = interactions.groupby(['User_ID', 'Vendor_ID'])['Interaction_Value'].sum().reset_index()\n",
        "\n",
        "# After grouping and summing interactions\n",
        "print(\"\\n=== Grouped Interactions ===\")\n",
        "print(\"Interaction grouped head:\")\n",
        "print(interaction_grouped.head())\n",
        "print(\"Shape:\", interaction_grouped.shape)\n",
        "print(\"Number of unique users:\", interaction_grouped['User_ID'].nunique())\n",
        "print(\"Number of unique vendors:\", interaction_grouped['Vendor_ID'].nunique())\n",
        "\n",
        "# Encode User_ID and Vendor_ID as integer indices\n",
        "user_encoder = LabelEncoder()\n",
        "vendor_encoder = LabelEncoder()\n",
        "\n",
        "interaction_grouped['user_index'] = user_encoder.fit_transform(interaction_grouped['User_ID'])\n",
        "interaction_grouped['vendor_index'] = vendor_encoder.fit_transform(interaction_grouped['Vendor_ID'])\n",
        "\n",
        "# Create lookup dictionaries for original IDs\n",
        "user_id_lookup = dict(zip(interaction_grouped['user_index'], interaction_grouped['User_ID']))\n",
        "vendor_id_lookup = dict(zip(interaction_grouped['vendor_index'], interaction_grouped['Vendor_ID']))\n",
        "\n",
        "# Create the sparse matrix (rows: users, cols: vendors)\n",
        "sparse_user_vendor = csr_matrix((\n",
        "    interaction_grouped['Interaction_Value'],\n",
        "    (interaction_grouped['user_index'], interaction_grouped['vendor_index'])\n",
        "))\n",
        "\n",
        "print(\"\\n=== Sparse Matrix Info ===\")\n",
        "print(\"Sparse matrix shape:\", sparse_user_vendor.shape)\n",
        "print(\"Number of non-zero elements:\", sparse_user_vendor.nnz)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3clL7uzIwA-j",
        "outputId": "3bd8f445-25ef-4971-e495-2cec0ce060f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Combined Interactions ===\n",
            "Merged Payment + Booking Sample:\n",
            "   User_ID  Vendor_ID  Interaction_Value\n",
            "0    64078       2542           32154.78\n",
            "1    67619       1715           28638.04\n",
            "2    76812        253           20587.46\n",
            "3    74231        177            9477.45\n",
            "4    44354       1525           19083.02\n",
            "Shape of interactions_df: (400000, 3)\n",
            "\n",
            "=== Grouped Interactions ===\n",
            "Interaction grouped head:\n",
            "   User_ID  Vendor_ID  Interaction_Value\n",
            "0        1        507           38718.21\n",
            "1        1       3106           45198.80\n",
            "2        2        484           40624.08\n",
            "3        2       2268           12221.97\n",
            "4        2       2848            5288.61\n",
            "Shape: (399830, 3)\n",
            "Number of unique users: 98144\n",
            "Number of unique vendors: 5000\n",
            "\n",
            "=== Sparse Matrix Info ===\n",
            "Sparse matrix shape: (98144, 5000)\n",
            "Number of non-zero elements: 399830\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cold Start Recommendation using Content-Based Filtering\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def handle_cold_start_new_user(user_profile, vendors_df, top_n=5):\n",
        "    \"\"\"\n",
        "    Recommend vendors to a new user based on their profile.\n",
        "    user_profile: dict with keys like 'interests', 'location', etc.\n",
        "    vendors_df: Vendors dataframe with 'services', 'category', 'location' columns\n",
        "    \"\"\"\n",
        "    vendors = vendors_df.copy()\n",
        "    vendors['features'] = (vendors['services'].fillna('') + ' ' + vendors['category'].fillna('') + ' ' + vendors['location'].fillna('')).str.lower()\n",
        "    user_text = (user_profile.get('interests', '') + ' ' + user_profile.get('location', '')).lower()\n",
        "\n",
        "\n",
        "    tfidf = TfidfVectorizer()\n",
        "    tfidf_matrix = tfidf.fit_transform(vendors['features'].tolist() + [user_text])\n",
        "\n",
        "    user_vec = tfidf_matrix[-1]\n",
        "    vendor_vecs = tfidf_matrix[:-1]\n",
        "\n",
        "    scores = cosine_similarity(user_vec, vendor_vecs).flatten()\n",
        "    top_indices = scores.argsort()[-top_n:][::-1]\n",
        "    return vendors.iloc[top_indices][['vendor_id', 'category', 'location', 'services']]\n",
        "\n",
        "def handle_cold_start_new_vendor(vendor_profile, users_df, top_n=5):\n",
        "    \"\"\"\n",
        "    Recommend users to a new vendor based on vendor profile.\n",
        "    vendor_profile: dict with keys like 'services', 'category', 'location'\n",
        "    users_df: Users dataframe with 'interests', 'location' columns\n",
        "    \"\"\"\n",
        "    users = users_df.copy()\n",
        "    users['features'] = (users['interests'].fillna('') + ' ' + users['location'].fillna('')).str.lower()\n",
        "    vendor_text = ( vendor_profile.get('services', '') + ' ' + vendor_profile.get('category', '') + ' ' + vendor_profile.get('location', '') ).lower()\n",
        "\n",
        "    tfidf = TfidfVectorizer()\n",
        "    tfidf_matrix = tfidf.fit_transform(users['features'].tolist() + [vendor_text])\n",
        "\n",
        "    vendor_vec = tfidf_matrix[-1]\n",
        "    user_vecs = tfidf_matrix[:-1]\n",
        "\n",
        "    scores = cosine_similarity(vendor_vec, user_vecs).flatten()\n",
        "    top_indices = scores.argsort()[-top_n:][::-1]\n",
        "    return users.iloc[top_indices][['user_id', 'interests', 'location']]\n",
        "\n"
      ],
      "metadata": {
        "id": "2ZCjqasZfObq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Prepare vendor features for LightFM ---\n",
        "print(\"\\nPreparing vendor features for LightFM...\")\n",
        "\n",
        "def prepare_vendor_features_lightfm(Vendors):\n",
        "    # 1. Identify categorical features if present\n",
        "    categorical_features = []\n",
        "    if 'Category' in Vendors.columns:\n",
        "        categorical_features.append('Category')\n",
        "    if 'Region' in Vendors.columns:\n",
        "        categorical_features.append('Region')\n",
        "\n",
        "    # Fill missing categorical with 'Unknown' to avoid issues later\n",
        "    Vendors[categorical_features] = Vendors[categorical_features].fillna('Unknown')\n",
        "\n",
        "    # 2. Select numerical features\n",
        "    vendor_features = Vendors.select_dtypes(include=[np.number])\n",
        "\n",
        "    # If no numerical features, add dummy feature\n",
        "    if vendor_features.empty:\n",
        "        vendor_features = pd.DataFrame({'dummy': np.ones(len(Vendors))}, index=Vendors.index)\n",
        "\n",
        "    # Normalize numerical features\n",
        "    scaler = StandardScaler()\n",
        "    vendor_features_scaled_array = scaler.fit_transform(vendor_features)\n",
        "    vendor_features_scaled = pd.DataFrame(vendor_features_scaled_array, index=Vendors['Vendor_ID'], columns=vendor_features.columns)\n",
        "\n",
        "    # 3. Bin numerical features into 5 bins and convert to categorical-like string features\n",
        "    binned_features = {}\n",
        "    for col in vendor_features_scaled.columns:\n",
        "       # Create 5 bins for each numerical feature\n",
        "       bins = pd.cut(vendor_features_scaled[col], bins=5, labels=False)\n",
        "       binned_features[col] = [f\"{col}_bin_{val}\" for val in bins]\n",
        "\n",
        "    binned_df = pd.DataFrame(binned_features, index=vendor_features_scaled.index)\n",
        "\n",
        "    # 4. Combine categorical and binned numerical features for each vendor\n",
        "    vendors_indexed = Vendors.set_index('Vendor_ID')\n",
        "\n",
        "    vendor_features_lightfm = []\n",
        "    for vendor_id in vendor_features_scaled.index:\n",
        "        features = []\n",
        "\n",
        "        # Add categorical features\n",
        "        for cat_col in categorical_features:\n",
        "            cat_val = vendors_indexed.loc[vendor_id, cat_col]\n",
        "            features.append(f\"{cat_col}_{cat_val}\")\n",
        "\n",
        "        # Add binned numerical features\n",
        "        for col in binned_df.columns:\n",
        "            features.append(binned_df.loc[vendor_id, col])\n",
        "\n",
        "        vendor_features_lightfm.append((vendor_id, features))\n",
        "\n",
        "    return vendor_features_lightfm\n",
        "\n",
        "# Usage:\n",
        "vendor_features_lightfm = prepare_vendor_features_lightfm(Vendors)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvViCaeTg9m3",
        "outputId": "a56c4824-3126-453e-cffd-34eb8af8e57a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preparing vendor features for LightFM...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Prepare LightFM dataset ---\n",
        "print(\"\\nPreparing LightFM dataset...\")\n",
        "dataset = Dataset()\n",
        "\n",
        "# Get unique users and vendors from your processed data\n",
        "unique_users = interaction_grouped['User_ID'].unique()\n",
        "unique_vendors = interaction_grouped['Vendor_ID'].unique()\n",
        "\n",
        "# Fit dataset with proper feature handling\n",
        "all_item_features = set()\n",
        "for _, features in vendor_features_lightfm:\n",
        "    all_item_features.update(features)\n",
        "\n",
        "dataset.fit(\n",
        "    users=unique_users,\n",
        "    items=unique_vendors,\n",
        "    item_features=list(all_item_features)\n",
        ")\n"
      ],
      "metadata": {
        "id": "RvQvz1Kkx26o",
        "outputId": "549398fc-431f-4eba-bef6-0aef0e89d862",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preparing LightFM dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build interactions matrix\n",
        "(interactions_matrix, weights) = dataset.build_interactions(\n",
        "    [(row['User_ID'], row['Vendor_ID'], row['Interaction_Value'])\n",
        "    for _, row in interaction_grouped.iterrows()\n",
        "])\n",
        "\n",
        "# Build item features matrix\n",
        "item_features = dataset.build_item_features(\n",
        "    [(vendor_id, features)\n",
        "     for vendor_id, features in vendor_features_lightfm\n",
        "     if vendor_id in unique_vendors]\n",
        ")\n",
        "\n",
        "print(\"\\nMatrix shapes:\")\n",
        "print(\"Interactions matrix:\", interactions_matrix.shape)\n",
        "print(\"Item features matrix:\", item_features.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LihJ_RHRgA9H",
        "outputId": "1f60b9fe-4be2-4ff9-db3b-6725ddceb9f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Matrix shapes:\n",
            "Interactions matrix: (98144, 5000)\n",
            "Item features matrix: (5000, 5022)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Split interactions using built-in LightFM method ---\n",
        "train_interactions, test_interactions = random_train_test_split(\n",
        "    interactions_matrix,\n",
        "    test_percentage=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# --- Hyperparameter Tuning Function ---\n",
        "def tune_lightfm(train, test, item_features):\n",
        "    # Define parameter grid\n",
        "    param_grid = {\n",
        "        'loss': ['warp', 'logistic'],\n",
        "        'no_components': [20, 30, 40],\n",
        "        'learning_rate': [0.01, 0.05],\n",
        "        'item_alpha': [1e-6, 1e-5],\n",
        "        'user_alpha' : [1e-6, 1e-5]\n",
        "    }\n",
        "\n",
        "    best_score = -1\n",
        "    best_params = {}\n",
        "    best_model = None\n",
        "\n",
        "    # Grid search\n",
        "    for loss in param_grid['loss']:\n",
        "        for n_comp in param_grid['no_components']:\n",
        "            for lr in param_grid['learning_rate']:\n",
        "                for item_alpha in param_grid['item_alpha']:\n",
        "                  for user_alpha in param_grid['user_alpha']:\n",
        "                    print(f\"\\nTesting config: loss={loss}, n_comp={n_comp}, lr={lr}, item_alpha={item_alpha}, user_alpha={user_alpha}\")\n",
        "\n",
        "                    model = LightFM(\n",
        "                        loss=loss,\n",
        "                        no_components=n_comp,\n",
        "                        learning_rate=lr,\n",
        "                        item_alpha=item_alpha,\n",
        "                        user_alpha=user_alpha,\n",
        "                        random_state=42\n",
        "                    )\n",
        "\n",
        "                    # Train with early stopping\n",
        "                    best_epoch_score = -1\n",
        "                    no_improvement = 0\n",
        "                    best_epoch_model = None\n",
        "\n",
        "                    for epoch in range(50):\n",
        "                        model.fit_partial(\n",
        "                            train,\n",
        "                            item_features=item_features,\n",
        "                            epochs=1,\n",
        "                            num_threads=4\n",
        "                        )\n",
        "\n",
        "                        # Use safe evaluation\n",
        "                        current_metrics = safe_evaluate(\n",
        "                            model,\n",
        "                            test,\n",
        "                            item_features\n",
        "                        )\n",
        "\n",
        "                        if current_metrics is None:\n",
        "                            print(\"Evaluation failed, skipping configuration\")\n",
        "                            break\n",
        "                        current_auc = current_metrics.get('auc', -1)\n",
        "                        print(f\"Epoch {epoch+1}: AUC = {current_auc:.4f}\", end='\\r')\n",
        "\n",
        "                        current_auc = auc_score(\n",
        "                            model,\n",
        "                            test,\n",
        "                            item_features=item_features\n",
        "                        ).mean()\n",
        "\n",
        "                        print(f\"Epoch {epoch+1}: AUC = {current_auc:.4f}\", end='\\r')\n",
        "\n",
        "                        # Early stopping\n",
        "                        if current_auc > best_epoch_score:\n",
        "                            best_epoch_score = current_auc\n",
        "                            no_improvement = 0\n",
        "                            best_epoch_model = model\n",
        "                        else:\n",
        "                            no_improvement += 1\n",
        "                            if no_improvement >= 5:\n",
        "                                model = best_epoch_model\n",
        "                                break\n",
        "\n",
        "                    if best_epoch_score > best_score and best_epoch_model:\n",
        "                        best_score = best_epoch_score\n",
        "                        best_params = {\n",
        "                            'loss': loss,\n",
        "                            'no_components': n_comp,\n",
        "                            'learning_rate': lr,\n",
        "                            'item_alpha': item_alpha,\n",
        "                            'user_alpha': user_alpha\n",
        "                        }\n",
        "                        best_model = best_epoch_model\n",
        "                        print(f\"\\nNew best config! AUC = {best_score:.4f}\")\n",
        "\n",
        "    return best_model, best_params\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Define model save path in Google Drive\n",
        "    model_path = '/content/drive/My Drive/AI/lightfm_model.pkl'\n",
        "    params_path = '/content/drive/My Drive/AI/lightfm_params.pkl'\n",
        "    metrics_path = '/content/drive/My Drive/AI/lightfm_metrics.pkl'\n",
        "\n",
        "    # Train or load model\n",
        "    if os.path.exists(model_path) and os.path.exists(params_path):\n",
        "        print(\"Loading saved model...\")\n",
        "        with open(model_path, 'rb') as f:\n",
        "            model = pickle.load(f)\n",
        "        with open(params_path, 'rb') as f:\n",
        "            best_params = pickle.load(f)\n",
        "        if os.path.exists(metrics_path):\n",
        "          with open(metrics_path, 'rb') as f:\n",
        "            final_metrics = pickle.load(f)\n",
        "    else:\n",
        "        print(\"Tuning hyperparameters...\")\n",
        "        model, best_params = tune_lightfm(\n",
        "            train_interactions,\n",
        "            test_interactions,\n",
        "            item_features\n",
        "        )\n",
        "\n",
        "        # Evaluate final model\n",
        "        final_metrics = safe_evaluate(\n",
        "            model,\n",
        "            test_interactions,\n",
        "            item_features=item_features\n",
        "        )\n",
        "\n",
        "        # Save best model and parameters\n",
        "        with open(model_path, 'wb') as f:\n",
        "            pickle.dump(model, f)\n",
        "        with open(params_path, 'wb') as f:\n",
        "            pickle.dump(best_params, f)\n",
        "        with open(metrics_path, 'wb') as f:\n",
        "            pickle.dump(final_metrics, f)\n",
        "\n",
        "    print(\"\\n=== Best Parameters ===\")\n",
        "    for k, v in best_params.items():\n",
        "        print(f\"{k}: {v}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPYy488bl-EM",
        "outputId": "75ca6b7d-477e-4c7c-8e9c-621cb858c8c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading saved model...\n",
            "\n",
            "=== Best Parameters ===\n",
            "loss: warp\n",
            "no_components: 30\n",
            "learning_rate: 0.01\n",
            "item_alpha: 1e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from lightfm.evaluation import precision_at_k, recall_at_k, auc_score\n",
        "import numpy as np\n",
        "\n",
        "def robust_evaluate(model, test_interactions, item_features=None, k=5):\n",
        "    \"\"\"\n",
        "    Ultra-robust evaluation that handles all edge cases\n",
        "    \"\"\"\n",
        "    metrics = {}\n",
        "\n",
        "    # Basic evaluation with item features\n",
        "    try:\n",
        "        print(\"\\nAttempting basic evaluation (with item features)...\")\n",
        "        metrics['precision'] = precision_at_k(\n",
        "            model,\n",
        "            test_interactions,\n",
        "            item_features=item_features,\n",
        "            user_features=None,\n",
        "            k=k\n",
        "        ).mean()\n",
        "\n",
        "        metrics['recall'] = recall_at_k(\n",
        "            model,\n",
        "            test_interactions,\n",
        "            item_features=item_features,\n",
        "            user_features=None,\n",
        "            k=k\n",
        "        ).mean()\n",
        "\n",
        "        metrics['auc'] = auc_score(\n",
        "            model,\n",
        "            test_interactions,\n",
        "            item_features=item_features,\n",
        "            user_features=None\n",
        "        ).mean()\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Basic evaluation failed: {str(e)}\")\n",
        "\n",
        "    # Fallback: Manual AUC calculation\n",
        "    try:\n",
        "        print(\"\\nAttempting manual AUC calculation...\")\n",
        "        test_csr = test_interactions.tocsr()\n",
        "        n_users, n_items = test_csr.shape\n",
        "        pos_scores = []\n",
        "        neg_scores = []\n",
        "\n",
        "        for user_id in range(n_users):\n",
        "            user_row = test_csr.getrow(user_id)\n",
        "            pos_item_indices = user_row.indices\n",
        "\n",
        "            if len(pos_item_indices) == 0:\n",
        "                continue\n",
        "\n",
        "            # Negative sampling: items the user did NOT interact with\n",
        "            neg_item_indices = np.setdiff1d(np.arange(n_items), pos_item_indices)\n",
        "\n",
        "            if len(neg_item_indices) == 0:\n",
        "                continue\n",
        "\n",
        "            pos_preds = model.predict(user_id, pos_item_indices, item_features=item_features)\n",
        "            neg_sample = np.random.choice(neg_item_indices, size=len(pos_item_indices), replace=True)\n",
        "            neg_preds = model.predict(user_id, neg_sample, item_features=item_features)\n",
        "\n",
        "            pos_scores.extend(pos_preds)\n",
        "            neg_scores.extend(neg_preds)\n",
        "\n",
        "        pos_scores = np.array(pos_scores)\n",
        "        neg_scores = np.array(neg_scores)\n",
        "\n",
        "        if len(pos_scores) > 0 and len(neg_scores) > 0:\n",
        "            auc = (pos_scores > neg_scores).mean()\n",
        "        else:\n",
        "            auc = np.nan\n",
        "\n",
        "        metrics['auc'] = auc\n",
        "        metrics['precision'] = np.nan\n",
        "        metrics['recall'] = np.nan\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Manual AUC calculation also failed: {str(e)}\")\n",
        "        return {\n",
        "            'precision': np.nan,\n",
        "            'recall': np.nan,\n",
        "            'auc': np.nan\n",
        "        }\n",
        "results = robust_evaluate(model, test_interactions, item_features=item_features, k=5)\n",
        "print(\"\\nEvaluation Results:\")\n",
        "print(f\"Precision@k: {results['precision']}\")\n",
        "print(f\"Recall@k: {results['recall']}\")\n",
        "print(f\"AUC: {results['auc']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdANRtpC7cRg",
        "outputId": "817c50ce-00b7-469a-ef2a-3802f35b9091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Attempting basic evaluation (with item features)...\n",
            "\n",
            "Evaluation Results:\n",
            "Precision@k: 0.0002724894729908556\n",
            "Recall@k: 0.0010324323015065639\n",
            "AUC: 0.5119229555130005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HwaSprXTm2di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_cold_start_new_user(user_id, top_n=5):\n",
        "    \"\"\"Enhanced cold start handling for new users\"\"\"\n",
        "    try:\n",
        "        # Get user preferences\n",
        "        user = Users[Users['User_ID'] == user_id].iloc[0]\n",
        "\n",
        "        # Create user profile\n",
        "        user_profile = {\n",
        "            'Event_Preference': user['Event_Preference'],\n",
        "            'Location': user['Location']\n",
        "        }\n",
        "\n",
        "        # Get vendors matching user preferences\n",
        "        matching_vendors = Vendors[\n",
        "            (Vendors['Category'] == user_profile['Event_Preference']) &\n",
        "            (Vendors['Rating'] >= 4.0)  # High-quality vendors\n",
        "        ]\n",
        "\n",
        "        # Sort by popularity score\n",
        "        matching_vendors = matching_vendors.sort_values('Popularity_Score', ascending=False)\n",
        "\n",
        "        return matching_vendors['Vendor_ID'].head(top_n).tolist()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in cold start handling: {str(e)}\")\n",
        "        return get_popular_vendors(top_n)"
      ],
      "metadata": {
        "id": "c00S8g2GknMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_popular_vendors(top_n=5):\n",
        "    \"\"\"Fallback function to return most popular vendors\"\"\"\n",
        "    popular = interaction_grouped['Vendor_ID'].value_counts().head(top_n).index.tolist()\n",
        "    return popular\n",
        "\n",
        "def get_similar_vendors(vendor_id, top_n=5):\n",
        "    \"\"\"Get content-based similar vendors\"\"\"\n",
        "    try:\n",
        "        if vendor_id not in vendor_features_scaled.index:\n",
        "            print(f\"Vendor {vendor_id} not found in features data.\")\n",
        "            return []\n",
        "\n",
        "        # Get vendor features\n",
        "        vendor_vec = vendor_features_scaled.loc[vendor_id].values.reshape(1, -1)\n",
        "        all_features = vendor_features_scaled.values\n",
        "\n",
        "        # Calculate cosine similarity\n",
        "        similarities = cosine_similarity(vendor_vec, all_features).flatten()\n",
        "\n",
        "        # Get most similar (excluding self)\n",
        "        similar_indices = np.argsort(-similarities)[1:top_n+1]\n",
        "        similar_vendors = vendor_features_scaled.index[similar_indices]\n",
        "\n",
        "        return list(similar_vendors)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error finding similar vendors: {str(e)}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "R5v58I8glZvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Recommendation Functions ---\n",
        "def hybrid_recommend(user_id, top_n=5, return_scores=False):\n",
        "    \"\"\"\n",
        "    Generate hybrid recommendations for a given user\n",
        "    Args:\n",
        "        user_id: The original user ID from your data\n",
        "        top_n: Number of recommendations to return\n",
        "        return_scores: If True, returns (vendors, scores) tuple\n",
        "    Returns:\n",
        "        List of recommended vendor IDs or (vendors, scores) tuple\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Get user index from LightFM's mapping\n",
        "        user_idx = dataset.mapping()[0][user_id]\n",
        "\n",
        "        # Get all vendor indices\n",
        "        all_vendor_indices = np.arange(interactions_matrix.shape[1])\n",
        "\n",
        "        # Generate prediction scores (combines collaborative and content-based)\n",
        "        scores = model.predict(\n",
        "            user_idx,\n",
        "            all_vendor_indices,\n",
        "            item_features=item_features\n",
        "        )\n",
        "\n",
        "        # Get top N vendor indices\n",
        "        top_indices = np.argsort(-scores)[:top_n]\n",
        "        # Map back to original vendor IDs\n",
        "        vendor_id_map = {v: k for k, v in dataset.mapping()[2].items()}\n",
        "        recommended_vendors = [vendor_id_map[idx] for idx in top_indices]\n",
        "        recommended_scores = scores[top_indices]\n",
        "\n",
        "        if return_scores:\n",
        "            return recommended_vendors, recommended_scores\n",
        "        return recommended_vendors\n",
        "\n",
        "    except KeyError:\n",
        "        print(f\"\\nUser {user_id} not found in training data (cold-start user).\")\n",
        "\n",
        "        # Cold start: prompt for preferences and recommend based on similarity\n",
        "        recommendations = prompt_for_preferences(top_n)\n",
        "\n",
        "        if return_scores:\n",
        "            return recommendations, [None] * len(recommendations)\n",
        "        return recommendations\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating recommendations: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# --- Example Usage ---\n",
        "if len(interaction_grouped) > 0:\n",
        "    # Get a sample user that exists in the data\n",
        "    sample_user = interaction_grouped['User_ID'].iloc[0]\n",
        "\n",
        "    print(f\"\\nGenerating recommendations for user {sample_user}...\")\n",
        "    recommendations, scores = hybrid_recommend(sample_user, top_n=5, return_scores=True)\n",
        "\n",
        "    print(\"\\nTop Recommendations:\")\n",
        "    for vendor, score in zip(recommendations, scores):\n",
        "        vendor_name = Vendors[Vendors['Vendor_ID'] == vendor]['Vendor_Name'].values[0] if 'Vendor_Name' in Vendors.columns else vendor\n",
        "        print(f\"- {vendor_name} (score: {score:.3f})\")\n",
        "    # Show similar vendors to the top recommendation\n",
        "    if recommendations:\n",
        "        print(f\"\\nVendors similar to {recommendations[0]}:\")\n",
        "        similar_vendors = get_similar_vendors(recommendations[0])\n",
        "        for vendor in similar_vendors:\n",
        "            vendor_name = Vendors[Vendors['Vendor_ID'] == vendor]['Vendor_Name'].values[0] if 'Vendor_Name' in Vendors.columns else vendor\n",
        "            print(f\"- {vendor_name}\")\n",
        "else:\n",
        "    print(\"\\nNo interactions found - cannot generate recommendations\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae1xxdfdm0wK",
        "outputId": "5cdf3ada-e728-4cc8-ffe7-f62debf52c1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating recommendations for user 1...\n",
            "\n",
            "Top Recommendations:\n",
            "- 4880 (score: 0.118)\n",
            "- 4648 (score: 0.115)\n",
            "- 4301 (score: 0.114)\n",
            "- 4124 (score: 0.112)\n",
            "- 4529 (score: 0.109)\n",
            "\n",
            "Vendors similar to 4880:\n",
            "Error finding similar vendors: name 'vendor_features_scaled' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_metrics(metrics):\n",
        "    \"\"\"Visualize evaluation metrics\"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Prepare data\n",
        "    metric_names = [k.replace('train_', '').replace('test_', '') for k in metrics.keys() if 'train' in k]\n",
        "    train_values = [v for k, v in metrics.items() if 'train' in k]\n",
        "    test_values = [v for k, v in metrics.items() if 'test' in k]\n",
        "\n",
        "    x = range(len(metric_names))\n",
        "\n",
        "    # Plot bars\n",
        "    plt.bar(x, train_values, width=0.4, label='Train', align='center')\n",
        "    plt.bar([i + 0.4 for i in x], test_values, width=0.4, label='Test', align='center')\n",
        "\n",
        "    # Customize plot\n",
        "    plt.xticks([i + 0.2 for i in x], metric_names, rotation=45)\n",
        "    plt.title('Model Performance Comparison')\n",
        "    plt.ylabel('Score')\n",
        "    plt.ylim(0, 1)\n",
        "    plt.legend()\n",
        "\n",
        "    # Add value labels\n",
        "    for i, v in enumerate(train_values):\n",
        "        plt.text(i - 0.1, v + 0.02, f\"{v:.2f}\", color='blue')\n",
        "    for i, v in enumerate(test_values):\n",
        "        plt.text(i + 0.3, v + 0.02, f\"{v:.2f}\", color='orange')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()  # Explicitly show the plot"
      ],
      "metadata": {
        "id": "7joDjhZnYMN_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}